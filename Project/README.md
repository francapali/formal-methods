# üéÆ Steam Retention Analytics
### User Behavior, Purchasing Patterns, and Retention Analysis

![Project Status](https://img.shields.io/badge/Status-Complete-green)
![Python](https://img.shields.io/badge/Language-Python-blue)
![Focus](https://img.shields.io/badge/Focus-E--commerce_%26_Retention-green)

## üìã Project Description
This project analyzes the "Steam Video Games" dataset using a management and E-commerce approach. Instead of simply ranking popular games, the focus is on understanding the **User Lifecycle** within the platform.

We examine the relationship between the act of purchasing and actual product usage (playtime) to identify patterns of **Customer Churn**, **User Engagement**, and **Buying Behavior**.

## üéØ Business Objectives
The project aims to answer the following key business questions:
* **Purchase-to-Play Ratio:** What is the conversion rate between buying a game and actually playing it?
* **Retention Analysis:** Is there a critical playtime threshold (e.g., "The Golden Hour") that predicts long-term loyalty?
* **Value Segmentation:** Can we segment users based on their value (e.g., "Collectors" who buy but don't play vs. "Hardcore Gamers")?

## üóÇÔ∏è Dataset
* **Source:** [Steam Video Games Dataset (Kaggle)](https://www.kaggle.com/datasets/tamber/steam-video-games)
* **Content:** Data regarding User IDs, Game Titles, Behaviors (purchase/play), and Total Playtime hours (where applicable).

## üöÄ Roadmap
1.  **Data Ingestion & Cleaning:** Handling null values and normalizing game titles.
2.  **Feature Engineering:** Creating metrics such as "Average Playtime per User," "Purchase-to-Play Ratio," and "Engagement Score."
3.  **Exploratory Analysis (EDA):** Visualizing playtime distributions and purchase habits.
4.  **Advanced Analysis:** User Segmentation (K-Means Clustering) and Churn probability.
5.  **Reporting:** Final dashboard with operational insights.

## üõ†Ô∏è Installation & Setup Guide
To reproduce the analysis and view the interactive dashboard, configure your environment by following these steps:
1. **Install Python Dependencies**: The project requires specialized libraries for Process Mining (**PM4Py**), data manipulation, and visualization. Install them using ```pip```, with the command ```pip install -r requirements.txt```.
2. **Install Graphviz**: To correctly visualize the Petri Net generated by the process discovery alghoritms, **Graphviz** must be installed on your operating system and added to your PATH. Use the official [download link](https://graphviz.org/download/) and follow the procedure for your operating system.
3. **Execution workflow**: Run the scripts in the following order to process the data and generate the models.
     1. *Data ingestion and cleaning*: Trasforms the raw dataset into an event log. Run the command ```python data_prep.py```.
     2. *Process Discovery*: Applies mining algorithms (**Alpha, Heuristic, Inductive**) and calculates quality metrics like Fitness and Precision. Run the command ```python process_discovery.py```.

**NOTE: Step 3 is only required to process and analyze the data again. You can skip this step and go straight to the next section if you just want to see our results.**

## üìä Launching the dashboard
The dashboard is built with **Streamlit** and allows for interactive exploration of KPIs and mining models. To start the dashboard, run the command ```streamlit run dashboard.py```. Once started, the dashboard will be available from your browser (usually the address is ```http://localhost:8501```, but the webpage should start authomatically) and you can navigate the different sections.
* **Project Overview**: General statistics, objectives, and tech stack details.
* **Process Models**: Technical comparison between Alpha, Heuristic, and Inductive Miner models.
* **Business KPIs**: Visual analysis of the "Backlog" phenomenon and "Time-to-Engagement" metrics.
* **AI Analyst (Chatbot)**: An interactive assistant that uses Inductive Miner logic to answer questions about churn, backlog patterns, and optimization strategies.

## üõ†Ô∏è Tech Stack
* **Language:** Python
* **Core Libraries:** Pandas, NumPy
* **Data Viz:** Matplotlib, Seaborn, Plotly (for interactive dashboards)
* **Analytics:** Cohort Analysis, Modified RFM Analysis (Recency, Frequency, Monetary - adapted for playtime)

---
*Project created for Formal Methods in CS - A.Y. 2025/2026*
